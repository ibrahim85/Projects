{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coleridge Rich Text Competition\n",
    "[Competition Link](https://coleridgeinitiative.org/richcontextcompetition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem: \n",
    "To automate the discovery of research datasets and the associated research methods and fields in social science research publications. Participants should use any combination of machine learning and data analysis methods to identify the datasets used in a corpus of social science publications and infer both the scientific methods and fields used in the analysis and the research fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach\n",
    "[Code](ColeridgePipeline.ipynb)\n",
    "\n",
    "We considered some approaches to tackle this problem. Among those, developing a very deep CNN based multiclass classifier for identification of all datasets seemed like a good candidate. But it would require retraining with inclusion of every new dataset, and to start with the publications were not even labelled with respect to all datasets. 2500 documents were labelled for about 1000 datasets. Then, we evaluated some document level similarity matching techiques but they did not localize exact mentions and would not identify mentons of unseen datsets, which were a requirement for this project.\n",
    "\n",
    "So we decided to go for a sentence level classification approach, where we evaluate sentences and classify them as a dataset mention based on their linguistic properties. Our aim was a robust generalization of a sentence that mentions dataset as opposed to normal sentences. Then we use docuement similarity techniques to see if the dataset mention refers to one of the known datsets or a new dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "[Complete Code](ColeridgePipeline.ipynb)\n",
    "\n",
    "- Remove refrences section\n",
    "- Parse document through Spacy Language Model\n",
    "    - Sentence segmentation\n",
    "    - Named Entity Recognition\n",
    "- Infer year of publication as being equal to maximum year in entities of type TIME\n",
    "- Filter the sentences to keep those which contain a Named entity of type Organization\n",
    "- Preprocess the filtered sentences\n",
    "    - Expand Abbreviations\n",
    "    - Remove punctuations\n",
    "    - Remove non albhabetical words\n",
    "    - Make lower case\n",
    "    - Perform word stemming\n",
    "    - Special mapping (e.g replace study by survey)\n",
    "- Pass sentences through trained Convolutional Neural Network Classifier\n",
    "    - outputs the probability of a sentence being a datset mention\n",
    "    - select senteces above a P-threshold to get candidate mentions\n",
    "- Document Similarity matching of candidate mentions and known datset titles\n",
    "    - TFIDF Ngram matrix of candidate mentions\n",
    "    - TFIDF Ngram matrix of known datsets\n",
    "    - Cosine Similarity calculation\n",
    "    - Geting datasets above S-threshhold\n",
    "    - Grouping similar datasets\n",
    "    - TFIDF similarity matching within each group and relevant mentions to select most similar datsets from a group, focusing on numeric indicators like version number and dates\n",
    "    - Computing Scores and saving results\n",
    "- Fields Identification Module\n",
    "    - Generate TFIDF Ngram matrix of publication\n",
    "    - Generate TFIDF Ngram matrix of the wikipedia articles of that all candidate fields\n",
    "    - Cosine similarity matching\n",
    "    - Assigning the most similar field label and saving results\n",
    "- Methods Identification Module\n",
    "    - Find the occurences and frequency of sage methods tokens in the publications\n",
    "    - Threshhold by minimum frequency\n",
    "    - Score by the frequency save results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The precision problem\n",
    "When run against a test sample and considering only 1028 labelled datsets database we were able to achieve a recall of about 90% but precision was only about 10%.\n",
    "\n",
    "We observe that there were two reasons behind this:\n",
    "\n",
    "#### 1) Versions of the Same Datsets\n",
    "\n",
    "Most of the datasets come in groups like:\n",
    "- 'Current Population Survey: Annual Demographic File, 1984',\n",
    "- 'Current Population Survey: Annual Demographic File, 1985',\n",
    "- 'Current Population Survey: Annual Demographic File, 1987',\n",
    "- 'Current Population Survey: Annual Demographic File, 1988',\n",
    "- 'Current Population Survey: Annual Demographic File, 1989',\n",
    "- 'Current Population Survey: Annual Demographic File, 1990'\n",
    "\n",
    "These groups can be larger than 50 in size. Since the titles for these groups are similar it is hard for the model to distinguish between them. We were able to narrow down to the right group in most cases but finding the right dataset presented a problem. If we return all these matches it results in a very bad precision and trying to filter among these sacrifices accuracy. We build a cascaded approach for this problem described below but still we were not able to deal effectively with this problem.\n",
    "\n",
    "#### 2) Primary Dataset in Labels\n",
    "Since the actuall labelling is done by the human expert he has some insights about the study and aonly includes the dataset in label if it is actually being used by the study. But some sentences mention datasets when referring to sister publications or discussing contemporary research efforts. Since we are solving the problem at sentence level we end up picking out these mentions as well which hurts our precision. We came up with a simple way to exclude references section from the publication before processing, but it needs to be matured further. So if we can develop methods to intelligently parse the publication into structuted text, and incorporate incorporation about where the mention came from in text, we can tackle this problem. E.g mentions in methods and dataset sections would be more relvant as compared to literature review and futre work sections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Dataset for Sentence Binary Classification:\n",
    "\n",
    "[Dataset Prep Notebook](SentenceClassificationDataExtraction.ipynb)\n",
    "\n",
    "We noticed that most of the dataset mentions had a named entity of type ORG in it, as detected by the SPACY language model. So if we extraxt all sentences that have a named entity in them we can drastically shorten the space for our search. Since if an article talks about a datasets, multiple sentecnces usually refer to it. So by filtering to sentences using NER will not hurt the recall much.\n",
    "\n",
    "- Positive Examples: \n",
    "All raw sentences that mention datasets. Mentions are extracted from citations file and complete respective sentences are extracted from raw text copurs. Complete sentences are used to provide model with context and help it generalize over the liguistic qualitites of a dataset mention.\n",
    "\n",
    "- Negative Examples: \n",
    "All raw sentences that contained a named entity but are not talking about datasets.\n",
    "\n",
    "#### Spacy was used for sentence segmentation and named entity recognition.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abbreviation Expansion\n",
    "[Abbreviation Expansion Notebook](AbbreviationDisambiguation.ipynb)\n",
    "\n",
    "We discovered that abbreviation expansion was extremely important becuase the text refers to datasets as abbreviations mostly. Abbreviation expansion module inhanced the accuracy of our sentence classification model by 7 %. When there are multiple possible expansions True expansion of the abbreviation should also take into consideration the context of the text. Otherwise it will lead to bias in the model. Example being WLS which can be 'Wisconsin Longitudinal Study' or 'Weighted Least Square'. We did not handle this for now but it is crucial in any future work.\n",
    "\n",
    "We developed a abbreviation expansion dictionary from the datsets metadata file. It was conststurcted using following approach:\n",
    "- All the abbreviations were detected using regex matching\n",
    "- N-Grams of the relevant length were extracted from the description an metions (both with and without stop word removal)\n",
    "- Abbreviation and intials of N-grams were matched to create possible expansions\n",
    "- Lemmatization resolved for the muliple expansion caused by  'X study' and 'X studies' \n",
    "- Manual pruning resolved the remaininig multiple expansions\n",
    "\n",
    "We also attempted to create an abbreviation dictionary for the whole corpus, but it resulted in a lot of bogus expansions that would induce bias in the model. So we did not use it. But it is certianly something that can be worked on in future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mention Classification Model:\n",
    "[Mention Classification Notebook](SentenceClassificationCNNModel.ipynb)\n",
    "\n",
    "This model was trained to take all sentences talking about a named entity and classify them as dataset mentions or not. The training pipeline was as follows:\n",
    "\n",
    "- Load Training and filter to sentences between 5 and 65 length.\n",
    "- Preprocess the Sentences\n",
    "    - Expand Abbreviations\n",
    "    - Remove punctuations\n",
    "    - Remove non albhabetical words\n",
    "    - Make lower case\n",
    "    - Perform word stemming\n",
    "    - Special mapping (e.g replace study by survey)\n",
    "- Build a vocabulary\n",
    "- Tokenize sentences into vectors\n",
    "- Train the model\n",
    "\n",
    "### Classifer\n",
    "ANN classifier was used for this purpose. We were inspired by the wide use of CNNs are widely used for document classification but also realized that LSTMs are better at modeling intricate linguistic qualities specially the ones with long range dependencies. Hence we tested both LSTMs and CNN for this task and CNNs gave us better results (I would like to state that our testing of these paradigms was not exhaustive in terms of achitecture and hyperparameters). We observed that our model tended to overfit very quickly so we had to limit training to a very few epochs and introduce strict dropout, becuase we wanted our model to generalize rather than learn the input data. We were able to achive good accuracy on our dataset after hyperparameter tuning and trying different data cleaning methods. Following are important observations:\n",
    "- abbreviation expansion module aimproved accuracy by 7 %\n",
    "- word stemming improved accuracy by 6 %\n",
    "- word2vec or GloVe word embeddings did not do well\n",
    "- training our own embedding layer with the model did well\n",
    "- We were able to get Validation Set accuracy of 94%\n",
    "- We believe that if large enough data is avalable fot training a very robust mention classifier can be trained\n",
    "\n",
    "### Model Architecture:\n",
    "After hyperparamter tuning the following architecture was selected:\n",
    "<img src=\"modelArchi.png\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidate Sentences \n",
    "The senteces that had probability greater than P_threshold were selected and passed further into the pipeline. The threshold value of 0.8 was used after tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NGram Similarity Candidate Mentions and Dataset Titles\n",
    "[Whole pipeline Notebook has relevant Code](Pipeline_Production.ipynb)\n",
    "\n",
    "We solved that problem by cascading two modules of coarse search and fine search. \n",
    "\n",
    "For selected candidate sentences we did following steps:\n",
    "- Clean sentences and use only alphabetic tokens\n",
    "- Generate TFIDF Ngram (length 2 to 4) matrix of sentences\n",
    "- Generate TFIDF Ngram (length 2 to 4) matrix of Datset titles\n",
    "- Compute Cosine Similarity\n",
    "- Coarse Search: Select datsets above a threshhold for each sentence\n",
    "- Merge similar datsets into groups (e.g different versions of same data)\n",
    "- Dataset group and group mention matching. For each dataset group:\n",
    "    - clean sentence and keep numeric features and dates\n",
    "    - expand dates (e.g 2012-2014 to 2012,2013,2014)\n",
    "    - Generate TFIDF matrix for group\n",
    "    - Generate TFIDF vector of all mentions appended together\n",
    "    - Cosine similarity matching\n",
    "    - Fine Search- select the dataset above a certain threshhold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Fields Identification\n",
    "[Feilds Extraction Wikipedia](WikipediaFiledExtraction.ipynb)  \n",
    "[Pipeline](Pipeline_Production.ipynb)\n",
    "\n",
    "Due to time constraints we were not able to fully explore the fields and methods identification problems. We decided to use document similarity matching between the publications and research fields. For that we tried to prepare text related to each field by extracting wikipedia articles related to that field. Some of the fields did not have an article that exactly matched the content. The field identification module have following steps:\n",
    "- Clean the text data\n",
    "- Compute TFIDF NGram matrices for both fields data and publications\n",
    "- Compute Cosine Similarity\n",
    "- Assign the field by maximum similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NGram Similarity Model\n",
    "\n",
    "[NGram Model Notebook](DatasetsDocNGRAMModel.ipynb)\n",
    "\n",
    "We evaluated the use of Ngram features and similarity matching as a way to create publication and datset pairs. The pipeline of the model was as follows:\n",
    "- Clean Data Set Titles and description and merge them into one sentence.\n",
    "- Preprocess the articles text and append it into a single line\n",
    "- Generate N-gram TFIDF matrix for datasets\n",
    "- Generate N-gram TFIDF matrix for articles\n",
    "- Compute Cosine similarity matrix\n",
    "- Filtering based on similarity value\n",
    "\n",
    "We got following results:\n",
    "- Selecting only the most similar datset: recall: 0.3 precision: 0.66\n",
    "- Selecting 50 most similar datsets for each publcication recall:0.95 precision:0.04\n",
    "- Selecting all the datasets above a threshHold value (tuned for maz F score): recall: 0.36 precision:0.52\n",
    "\n",
    "Following experiment only considered 1028 datsets search which were labelled in 2500. The performance might detereorate when all 10000 datsets are used. \n",
    "\n",
    "We did not pursue this approach further for following reasons:\n",
    "- It did not solve the second aspect of the problem where you were supposed to detect mentions for datsets not in database.\n",
    "- It was a document level matching so it did not pin point the exact mention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources at [Jason Brownlee's excellent website](https://machinelearningmastery.com) were a huge help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
