{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This script prepares data of positive and negative examples to train the classifier to differentiate between sentences that mention a data set and sentences which dont mention a dataset.\n",
    "\n",
    "### Positive Examples: All raw sentences that mention datasets. Mentions are extracted from citations file and complete relevant sentences are extracted from raw text copurs. Complete sentences are used to provide model with context and help it generalize over the liguistic qualitites of a dataset mention.\n",
    "\n",
    "### Negative Examples: All raw sentences that contained a named entity but are not talking about datasets.\n",
    "\n",
    "#### Spacy was used for sentence segmentation ang named entity recognition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## text file containing name of files which were labelled as containing datasets\n",
    "LABELLED_TEXT_FILES = 'labelledTextFiles.txt'\n",
    "txtdirectory = '/home/urwa/Documents/Coleridge/ProjectFiles/train_test/train_test/files/text/'\n",
    "CITATTIONS_FILE = 'data_set_citations.json'\n",
    "citations_directory = '/home/urwa/Documents/Coleridge/ProjectFiles/train_test/train_test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Load the large English NLP model\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save list to file\n",
    "def save_list(lines, filename):\n",
    "\tdata = '\\n'.join(lines)\n",
    "\tfile = open(filename, 'w')\n",
    "\tfile.write(data)\n",
    "\tfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2108, 2272, 1609, 589, 206]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#txtFiles = load_doc('sampleTextFiles.txt').split('\\n')\n",
    "txtFiles = load_doc(LABELLED_TEXT_FILES).split('\\n')\n",
    "# extrac publication IDs from text file name\n",
    "pub_ids = [int(t.split('.')[0]) for t in txtFiles]\n",
    "pub_ids[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading citations data from json file\n",
    "text = load_doc(citations_directory+CITATTIONS_FILE)\n",
    "cit = json.loads(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cell below divides the citation data in test and train in order to test if model is good at generalizing dataset mentions and correctly classifying them when they talk about unseen datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting unique dataset Ids\n",
    "datasetIds = list(set([ d['data_set_id'] for d in cit ]))\n",
    "\n",
    "#splitting into test train\n",
    "dataIdTrain = datasetIds[150:]\n",
    "dataIdTest = datasetIds[:150]\n",
    "\n",
    "# Split the citation information into test and train\n",
    "citTrain = [c for c in cit if c['data_set_id'] in dataIdTrain]\n",
    "citTest = [c for c in cit if c['data_set_id'] in dataIdTest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract dataset mention from json \n",
    "def getMentionsAll(file):\n",
    "    pid = int(file.split('.')[0])\n",
    "    citations = [c for c in cit if c['publication_id'] == pid]\n",
    "    mentions = []\n",
    "    for c in citations:\n",
    "        mentions += c['mention_list']\n",
    "    return( list(set(mentions)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract dataset mention from json \n",
    "def getMentionsTest(file):\n",
    "    pid = int(file.split('.')[0])\n",
    "    citations = [c for c in citTest if c['publication_id'] == pid]\n",
    "    mentions = []\n",
    "    for c in citations:\n",
    "        mentions += c['mention_list']\n",
    "    return( list(set(mentions)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract dataset mention from json \n",
    "def getMentionsTrain(file):\n",
    "    pid = int(file.split('.')[0])\n",
    "    citations = [c for c in citTrain if c['publication_id'] == pid]\n",
    "    mentions = []\n",
    "    for c in citations:\n",
    "        mentions += c['mention_list']\n",
    "    return( list(set(mentions)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMentionSentences(mentions,sentences):\n",
    "    mention_sentences = []\n",
    "    for m in mentions:\n",
    "        mention_sentences += [s for s in sentences if m in s.text.replace('\\n',' ')] \n",
    "    return list(set(mention_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def containsEntity(entities, sentence):\n",
    "    for e in entities:\n",
    "        if e.start >= sentence.start and e.end <= sentence.end:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell will preperare positive and negative examples and save them in files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mention_sentences = []\n",
    "all_entity_sentences = []\n",
    "i = 0\n",
    "for file in txtFiles:\n",
    "    i += 1\n",
    "    if(i % 100 == 0):\n",
    "        print(i)\n",
    "    txt = load_doc(txtdirectory+file)\n",
    "    doc = nlp(txt)\n",
    "    sentences = list(doc.sents)\n",
    "    entities = [e for e in doc.ents if e.label_ == 'ORG']\n",
    "    mentions = getMentionsAll(file)\n",
    "    \n",
    "    mentionSentences = getMentionSentences(mentions,sentences)\n",
    "    otherSentences = [s for s in sentences if s not in mentionSentences]\n",
    "    \n",
    "    entitySentences = [s for s in otherSentences if containsEntity(entities, s)]\n",
    "    \n",
    "    all_mention_sentences += [s.text for s in mentionSentences]\n",
    "    all_entity_sentences += [s.text for s in entitySentences]\n",
    "    \n",
    "    \n",
    "all_mention_sentences = [s.replace('\\n',' ') for s in all_mention_sentences]\n",
    "\n",
    "save_list(all_mention_sentences, 'all_positive_sentences.txt')\n",
    "save_list(all_entity_sentences, 'all_negative_sentences.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell will preperare positive test and train examples and save them in files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mention_sentences_train = []\n",
    "all_mention_sentences_test = []\n",
    "#all_entity_sentences = []\n",
    "i = 0\n",
    "for file in txtFiles:\n",
    "    i += 1\n",
    "    if(i % 100 == 0):\n",
    "        print(i)\n",
    "    txt = load_doc(txtdirectory+file)\n",
    "    doc = nlp(txt)\n",
    "    sentences = list(doc.sents)\n",
    "    #entities = [e for e in doc.ents if e.label_ == 'ORG']\n",
    "    mentions_train = getMentionsTrain(file)\n",
    "    mentions_test = getMentionsTest(file)\n",
    "    \n",
    "    mentionSentences_train = getMentionSentences(mentions_train,sentences)\n",
    "    mentionSentences_test = getMentionSentences(mentions_test,sentences)\n",
    "    otherSentences = [s for s in sentences if s not in mentionSentences]\n",
    "    \n",
    "    entitySentences = [s for s in otherSentences if containsEntity(entities, s)]\n",
    "    \n",
    "    all_mention_sentences_train += [s.text for s in mentionSentences_train]\n",
    "    all_mention_sentences_test += [s.text for s in mentionSentences_test]\n",
    "    #all_entity_sentences += [s.text for s in entitySentences]\n",
    "    \n",
    "    \n",
    "all_mention_sentences_train = [s.replace('\\n',' ') for s in all_mention_sentences_train]\n",
    "all_mention_sentences_test = [s.replace('\\n',' ') for s in all_mention_sentences_test]\n",
    "\n",
    "save_list(all_mention_sentences_train, 'all_train_positive_sentences2.txt')\n",
    "save_list(all_mention_sentences_test, 'all_test_positive_sentences2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
